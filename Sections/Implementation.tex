\section{ULFM and Fenix}\label{sec:implem}
The ULFM proposal adds a small set of functions to the MPI standard.
They enable users to return a program to a consistent state after a failure occurs.
The ULFM functions of use to Fenix are \texttt{MPI\_Comm\_shrink},
\texttt{MPI\_Comm\_revoke}, and \texttt{MPI\_Comm\_agree}.
These functions are invoked only when an error occurs, and hence do
not add the the MPI critical path.
However, ULFM requires that each non-local MPI call check an error code at least once to
monitor the occurrence of communication or communicator failures.
This overhead itself is strictly local, which means it is constant, regardless of the
size of the communicator involved, or the network distance between any communicating rank(s).
It is the only overhead that may affect performance of applications using
ULFM-enabled MPI.

Fenix is a library \cite{fenixspec} that conveniently supports MPI process,
as well as data recovery. In this paper we focus on the former.
Fenix is initialized with a call to \texttt{Fenix\_Init}, in which the user specifies,
among others, how many ranks should be kept in reserve, to be used to substitute for
defunct ranks after a failure; these ranks stay coralled inside texttt{Fenix\_Init}
until further notice.
Users also specify an input communicator (usually \texttt{MPI\_COMM\_WORLD}), as well
as a resilient output communicator.
If the latter is left blank, Fenix will create an anonymous one, and will tacitly
replace the communicator in any MPI call that matches the input communicator with
the anonymous output communicator.
When a failure occurs, control is transferred back to \texttt{Fenix\_Init}, which patches
the resilient communicator with reserved ranks, utilizing, among others, functions
provided by ULFM.
It also automatically deletes any communicators that were derived from the resilient output
communicator, and removes pending communication requests.
These processes only occur in case of failure, so they do not increase MPI's critical
path, although it is, of course, very important to make them as efficient as possible.
However, other operations that Fenix carries out do add to the critica path.

Fenix uses the MPI profiling interface (PMPI) to
perform one or more of the following tasks whenever a non-trivial MPI call is made:
\begin{enumerate}
\item check whether the library has been initialized.\label{item:init}
\item check whether the communicator named in the operation should be replaced with
  the anonymous output communicator.\label{item:anon}
\item check whether the MPI call has generated an error code corresponding to a
  communication or communicator failure.\label{item:procerr}
\item record a newly derived communicator in an internally maintained data structure.\label{item:derive}
\item record any created communication request in an internally maintained data structure.\label{item:record}
\item remove such a requestt from that data structure upon completion of the request.\label{item:clear}
\end{enumerate}
Of these, task \ref{item:derive} is normally not important for performance, since
in most HPC codes communicator creation, which is relatively expensive, is infrequent.
Tasks \ref{item:init}--\ref{item:procerr} are simple scalar tests of one statement each.
Tasks \ref{item:record} and \ref{item:clear} require accessing non-trivial data structures,
which could be more time-consuming.


