\section{Conclusions}
Our experiments show that even for very fine-grain HPC codes, overheads
incurred by ULFM and Fenix over the baseline OpenMPI implementation
are negligible in the absence of failures.
Moreover, they demonstrate that Fenix is applicable to tightly-coupled,
bulkp-synchronous applications that are common in high-performance
scientific computing.
Slightly modified versions of the Parallel Research Kernels were
instrumental in drawing these conclusions.
